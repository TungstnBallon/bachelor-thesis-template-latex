@article{ma:2013:commercial,
	volume = {56},
	title = {How commercial involvement affects open source projects: three
	         case studies on issue reporting},
	year = {2013},
	pages = {1--13},
	number = {8},
	journal = {Science China Information Sciences},
	publisher = {Springer},
	author = {Ma, XiuJuan and Zhou, MingHui and Riehle, Dirk},
}

@incollection{weikert:2013:model,
	publisher = {Springer},
	pages = {90--101},
	author = {Weikert, Florian and Riehle, Dirk},
	booktitle = {Software Business. From Physical Products to Software
	             Services and Solutions},
	year = {2013},
	title = {A Model of Commercial Open Source Software Product Features},
}

@article{riehle:2011:controlling,
	author = {Riehle, Dirk},
	title = {Controlling and Steering Open Source Projects},
	journal = {Computer},
	pages = {93--96},
	year = {2011},
	publisher = {IEEE Computer Society},
}

@article{riehle:2007:economic,
	volume = {40},
	pages = {25--32},
	journal = {Computer},
	author = {Riehle, Dirk},
	title = {The economic motivation of open source software: Stakeholder
	         perspectives},
	year = {2007},
	publisher = {IEEE},
	number = {4},
}

@dataset{dataset,
	author = {Ankur Napa},
	title = {Brewery Operations and Market Analysis},
	year = {2023},
	url = {
	       https://www.kaggle.com/datasets/ankurnapa/brewery-operations-and-market-analysis-dataset/data
	       },
	urldate = {2024-7-12},
	language = {english},
}

@online{jvalue:landing,
	url = {https://jvalue.com/},
	urldate = {2024-7-13},
}

@online{jvalue:jayvee,
	url = {https://jvalue.com/jayvee},
	urldate = {2024-7-13},
}

@article{Ahmad2020,
	author = {Tanveer Ahmad and Nauman Ahmed and Zaid Al-Ars and H. Peter
	          Hofstee},
	title = {Optimizing performance of GATK workflows using Apache Arrow
	         In-Memory data framework},
	journal = {BMC Genomics},
	year = {2020},
	month = {Nov},
	day = {18},
	volume = {21},
	number = {10},
	pages = {683},
	abstract = {Immense improvements in sequencing technologies enable
	            producing large amounts of high throughput and cost effective
	            next-generation sequencing (NGS) data. This data needs to be
	            processed efficiently for further downstream analyses.
	            Computing systems need this large amounts of data closer to
	            the processor (with low latency) for fast and efficient
	            processing. However, existing workflows depend heavily on
	            disk storage and access, to process this data incurs huge
	            disk I/O overheads. Previously, due to the cost, volatility
	            and other physical constraints of DRAM memory, it was not
	            feasible to place large amounts of working data sets in
	            memory. However, recent developments in storage-class memory
	            and non-volatile memory technologies have enabled computing
	            systems to place huge data in memory to process it directly
	            from memory to avoid disk I/O bottlenecks. To exploit the
	            benefits of such memory systems efficiently, proper formatted
	            data placement in memory and its high throughput access is
	            necessary by avoiding (de)-serialization and copy overheads
	            in between processes. For this purpose, we use the newly
	            developed Apache Arrow, a cross-language development
	            framework that provides language-independent columnar
	            in-memory data format for efficient in-memory big data
	            analytics. This allows genomics applications developed in
	            different programming languages to communicate in-memory
	            without having to access disk storage and avoiding
	            (de)-serialization and copy overheads.},
	issn = {1471-2164},
	doi = {10.1186/s12864-020-07013-y},
	url = {https://doi.org/10.1186/s12864-020-07013-y},
}
@article{Peltenburg2021,
	author = {Johan Peltenburg and Jeroen van Straten and Matthijs Brobbel
	          and Zaid Al-Ars and H. Peter Hofstee},
	title = {Generating High-Performance FPGA Accelerator Designs for Big
	         Data Analytics with Fletcher and Apache Arrow},
	journal = {Journal of Signal Processing Systems},
	year = {2021},
	month = {May},
	day = {01},
	volume = {93},
	number = {5},
	pages = {565-586},
	abstract = {As big data analytics systems are squeezing out the last
	            bits of performance of CPUs and GPUs, the next near-term and
	            widely available alternative industry is considering for
	            higher performance in the data center and cloud is the FPGA
	            accelerator. We discuss several challenges a developer has to
	            face when designing and integrating FPGA accelerators for big
	            data analytics pipelines. On the software side, we observe
	            complex run-time systems, hardware-unfriendly in-memory
	            layouts of data sets, and (de)serialization overhead. On the
	            hardware side, we observe a relative lack of
	            platform-agnostic open-source tooling, a high design effort
	            for data structure-specific interfaces, and a high design
	            effort for infrastructure. The open source Fletcher framework
	            addresses these challenges. It is built on top of Apache
	            Arrow, which provides a common, hardware-friendly in-memory
	            format to allow zero-copy communication of large tabular data
	            , preventing (de)serialization overhead. Fletcher adds FPGA
	            accelerators to the list of over eleven supported software
	            languages. To deal with the hardware challenges, we present
	            Arrow-specific components, providing easy-to-use,
	            high-performance interfaces to accelerated kernels. The
	            components are combined based on a generic architecture that
	            is specialized according to the application through an
	            extensive infrastructure generation framework that is
	            presented in this article. All generated hardware is
	            vendor-agnostic, and software drivers add a platform-agnostic
	            layer, allowing users to create portable implementations.},
	issn = {1939-8115},
	doi = {10.1007/s11265-021-01650-6},
	url = {https://doi.org/10.1007/s11265-021-01650-6},
}

@article{Dremio,
	title = {It’s Time to Replace ODBC \& JDBC},
	author = {Tomer Shiran},
	date = {2019-07-03},
	url = {https://www.dremio.com/blog/is-time-to-replace-odbc-jdbc},
	urldate = {2024-07-13},
}
@inbook{Floratou2019,
	author = "Floratou, Avrilia",
	editor = "Sakr, Sherif and Zomaya, Albert Y.",
	title = "Columnar Storage Formats",
	bookTitle = "Encyclopedia of Big Data Technologies",
	year = "2019",
	publisher = "Springer International Publishing",
	address = "Cham",
	pages = "464--469",
	isbn = "978-3-319-77525-8",
	doi = "10.1007/978-3-319-77525-8_248",
	url = "https://doi.org/10.1007/978-3-319-77525-8_248",
}
@online{arrow:status,
	url = {https://arrow.apache.org/docs/status.html},
	urldate = {2024-07-14},
}
@online{arrow:projects,
	url = {https://arrow.apache.org/powered_by/},
	urldate = {2024-07-14},
}
@inproceedings{Grossman2022,
	author = "Max Grossman and Steve Poole and Howard Pritchard and Vivek
	          Sarkar",
	editor = "Stephen Poole and Oscar Hernandez and Matthew Baker and Tony
	          Curtis",
	title = "SHMEM-ML: Leveraging OpenSHMEM and Apache Arrow for Scalable,
	         Composable Machine Learning",
	booktitle = "OpenSHMEM and Related Technologies. OpenSHMEM in the Era of
	             Exascale and Smart Networks",
	year = "2022",
	publisher = "Springer International Publishing",
	address = "Cham",
	pages = "111--125",
	abstract = "SHMEM-ML is a domain specific library for distributed array
	            computations and machine learning model training {\&}
	            inference. Like other projects at the intersection of machine
	            learning and HPC (e.g. dask, Arkouda, Legate Numpy), SHMEM-ML
	            aims to leverage the performance of the HPC software stack to
	            accelerate machine learning workflows. However, it differs in
	            a number of ways.",
	isbn = "978-3-031-04888-3",
	doi = {10.1007/978-3-031-04888-3_7},
	url = {https://doi.org/10.1007/978-3-031-04888-3_7},
}
@inproceedings{Furche2016,
	title = "Data Wrangling for Big Data: Challenges and Opportunities",
	abstract = "Data wrangling is the process by which the data required by
	            an applicationis identified, extracted, cleaned and
	            integrated, to yield adata set that is suitable for
	            exploration and analysis. Although thereare widely used
	            Extract, Transform and Load (ETL) techniques andplatforms,
	            they often require manual work from technical and
	            domainexperts at different stages of the process. When
	            confrontedwith the 4 V{\textquoteright}s of big data (volume,
	            velocity, variety and veracity),manual intervention may make
	            ETL prohibitively expensive. Thispaper argues that providing
	            cost-effective, highly-automated approachesto data wrangling
	            involves significant research challenges,requiring
	            fundamental changes to established areas such as data
	            extraction,integration and cleaning, and to the ways in which
	            theseareas are brought together. Specifically, the paper
	            discusses the importanceof comprehensive support for context
	            awareness withindata wrangling, and the need for adaptive,
	            pay-as-you-go solutionsthat automatically tune the wrangling
	            process to the requirementsand resources of the specific
	            application.",
	author = "Tim Furche and George Gottlob and Leonid Libkin and Giorgio
	          Orsi and Norman Paton",
	year = "2016",
	month = nov,
	day = "1",
	doi = "10.5441/002/edbt.2016.44",
	language = "English",
	isbn = "2367-2005",
	pages = "473--478",
	booktitle = "Advances in Database Technology — EDBT 2016",
}
@book{Rupp2020,
	title = "Requirements-Engineering und -Management",
	titleaddon = {Das Handbuch für Anforderungen in jeder Situation},
	author = "Christine Rupp and SOPHISTen",
	publisher = "Carl Hanser Verlag",
	month = dec,
	year = 2020,
	address = "Munich, Germany",
	language = "de",
}
