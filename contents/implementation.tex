\chapter{Implementation}
\label{chapter:Implementation}

\section{Type Conversion}
\label{subsection:TypeConversion}
In order to convert Polars' DataType to a Jayvee ValueType, the following method is added is added to the \Verb|ValueTypeProvider| class.
\mint{typescript}|fromPolarsDType(dtype: polars.DataType): ValueType|
This method uses \Verb|DataType.equals| to compare DataTypes.
Many DataTypes result in the same ValueType, for example, both \Verb|polars.Float32| and \Verb|polars.Float64| return \Verb|Decimal|.
The DataTypes \Verb|Categorical|, \Verb|Date|, \Verb|DateTime|, \Verb|List|, \Verb|Null| and \Verb|Struct| do not have a direct ValueType equivalent and throw an error.

To convert in the opposite direction, the \mint{typescript}{toPolarsDataType(): polars.DataType | undefined} method is added to the \Verb|ValueType| interface.
Unsupported ValueTypes are \Verb|EmptyCollection|, \Verb|Collection| if the inner ValueType is supported, \Verb|ValueTypeAssignment|, \Verb|Transform|, \Verb|Regex|, \Verb|Constraint|, \Verb|CellRange|.
Their implementation of \Verb|toPolarsDataType| returns \Verb|undefined|.

Often polars methods return \Verb|any| where the interpeter expects \Verb|InternalValueRepresentation|.
To narrow down the \Verb|any| type we expand the existing typeguards defined in \Verb|libs/language-server/src/lib/ast/expressions/typeguards.ts| to handle inputs with \Verb|unknown| type.
\Verb|string|, \Verb|number| and \Verb|boolean| use \Verb|typeof|.
\Verb|RegExp| uses \Verb|instanceof|.
\Verb|CellRangeLiteral|, \Verb|ConstraintDefinition|, \Verb|ValuetypeAssignment|, \Verb|BlockTypeProperty| and \Verb|TransformDefinition| use langium generated typeguards.
\Verb|AtomicInternalValueRepresentation| returns true if any of the above returns true.
\Verb|Array<InternalValueRepresentation>| uses \Verb|Array.isArray| and verifies every value of the array to be \Verb|InternalValueRepresentation|.
\Verb|InternalValueRepresentation| is true if either \Verb|Array<InternalValueRepresentation>| or \Verb|AtomicInternalValueRepresentation|.

\section{Table}
\subsection{abstract Table}
The table implementation is located in \Verb{libs/execution/src/lib/types/io-types/table.ts}.
To increase clarity, the abstract \Verb|TableColumn| class and its subclasses \Verb|PolarsTableColumn| and \Verb|TsTableColumn| are moved into their own file \Verb|table-column.ts|.

\subsubsection{changes compared to old table}
\label{subssubsection:impl:old_vs_new}
The old \Verb|addColum| method used to modify its table instance to add the column.
The new \Verb|withColum| method returns a new Table instance, that includes the new column.
However, the interpreter has to keep using \Verb|addColumn| when working with \Verb|TsTable|, because other behavoir would skew the evaluation results.

- leads to less mutations, so it's better for parallel execution %TODO: citation

\subsection{PolarsTable}
As described in \ref{subsection:arch:polarstable}, follows the adapter pattern and \Verb|PolarsTable| wraps \Verb|polars.DataFrame|.
This leads to several wrapper methods, that call \Verb|DataFrame| methods and return that result:
\Verb|writeIpc|, \Verb|writeIpcTo|, \Verb|withColumn|, \Verb|withColumnFromInternal|, \Verb|nRows|, \Verb|nColumns|, \Verb|clone| and \Verb|toString|.

\Verb|getTypes| gets the \Verb|DataFrame|'s types, an array of DataTypes, and applies \Verb|fromPolarsDType| (\ref{subsection:TypeConversion}).

\Verb|generateInsertValuesStatement| converts the dataframe to a row-oriented, two-dimensional array, containing \Verb|any| values.
Utilise the new typeguards descibed in \ref{subsection:TypeConversion}, to ensure these are values of type \Verb|InternalValueRepresentation|.
Convert each cell into its \ac{SQL} value with \Verb|SQLValueRepresentationVisitor|.
If the cell has no \ac{SQL} representation, use \Verb|'NULL'|.
\mintinline{typescript}|Array<string>.join| and template strings are used to combine the cells into rows and the rows into the final statement.

\Verb|generateCreateTableStatement| uses \Verb|SQLColumnTypeVisitor| to convert ValueTypes to \ac{SQL}, e. g. \Verb|decimal| becomes \Verb|'real'|.

\Verb|hasColumn| does not have a direct equivalent \Verb|DataFrame| method.
The method uses \Verb|DataFrame.getColumn| and returns \Verb|true| if the call succeeds, \Verb|false| if it throws an exception.

\Verb|columns| and \Verb|getColumn| both face the challenge, that \Verb|Series|, which is returned by \Verb|DataFrame.getColumn|, does not include the ValueType required by the \Verb|PolarsTableColumn| constructor.
They solve this by using the \Verb|this.valueTypeProvider.fromPolarsDType| conversion method.

\subsubsection{TsTable}
We will only describe new functionality, as this class only exists to preserve the original implementaiton.

\Verb|withColumn| uses the \Verb|clone| method and then adds the column to that new table using \Verb|addColumn|.

\Verb|clone| clones every column individually, puts them into a new map, and then wraps that map in a new \Verb|TsTable| object.

\section{TableColumn}
\subsection{polarsTableColumn}
\Verb|PolarsTableColumn| is a thin wrapper around \Verb|polars.Series|.
It does't have any methods, that aren't getters or wrappers around equivalent \Verb|Series| methods.

\subsubsection{TsTableColumn}
All properties are prefixed with an underscore to prevent a naming conflict between the property and their getter / setter.

\Verb|clone| requries a \emph{deep clone} of the \Verb|_values| array.
The usual way to accomplish this is the global \Verb|structuredClone| function.
This approach caused the program to crash at runtime.
We suspect the cause has to do with limitations of \Verb|structuredClone| described by \textcite{js:docs:structuredClone}.
This problem was solved by serializing the column to \ac{JSON}, and parsing it again, creating deep copies in the process.
The typegards from \ref{subsection:TypeConversion} were used to regain the type infromation lost by this process.

\Verb|push| checks if the new element is a valid for the column's ValueType using \Verb|ValueType.isInternalValueRepresentation| before pushing.

\Verb|drop| uses \Verb|Array<T>.splice| to remove one element from the value array.


\section{new executors} %FIXME
\subsection{Selecting the correct block executor} %FIXME
\label{subsection:createBlockExecutor}
\Verb|getExecutorForBlockType| in \Verb|libs/execution/src/lib/extension.ts| is modified to look for a different blocktype than its parameter specifies.
Refer to \ref{fig:uml:getExecutorForBlockType} for a diagram. %FIXME: what diagram
\begin{figure}
	\begin{plantuml}
		@startuml
		!pragma useVerticalIf on
		start
		if (blockTypeName === 'TableInterpreter') then (true)
		if (usePolars) then (true)
		:blockTypeName = 'PolarsTableInterpreter';
		else (false)
		:blockTypeName = 'TsTableInterpreter';
		endif
		elseif (blockTypeName === 'TableTransformer') then (true)
		if (usePolars) then (true)
		:blockTypeName = 'PolarsTableTransformer';
		else (false)
		:blockTypeName = 'TsTableTransformer';
		endif
		elseif (blockTypeName === 'SQLiteLoader') then (true)
		if (useRusqlite) then (true)
		:blockTypeName = 'RustTableTransformer';
		elseif (usePolars) then (true)
		:blockTypeName = 'PolarsTableTransformer';
		else (false)
		:blockTypeName = 'TsTableTransformer';
		endif
		endif
		stop
		@enduml
	\end{plantuml}
	\caption{How we pick the right blocktype name}
	\label{fig:uml:getExecutorForBlockType}
\end{figure}


\subsection{TableInterpreterExecutor} %TODO
\label{section:tableinterpreterexecutor}

\Verb|TableInterpreterExecutor| is implemented in \Verb|libs/extensions/tabular/exec/src/lib/table-interpreter-executor.ts|.
This file also contains the function
\mint{typescript}|toPolarsDataTypeWithLogs(valueType: ValueType, logger: Logger): polars.DataType|
Its purpose is to convert Polars DataTypes to Jayvee ValueTypes, replacing unsupported ValueTypes with a fallack (\ref{lst:toPolarsDataTypeWithLogs}).
log unsupported ValueTypes with \Verb|logger.logErr| and return the fallback \Verb|pl.Utf8|.
TODO?: circular imports?

\begin{listing}
	\begin{minted}{python}
def toPolarsDataTypeWithLogs(valueType, logger):
	dataType = valueType.toPolarsDataType()
	if dataType is undefined:
		... # Log the error using logger
		return polars.Utf8
	return dataType
	\end{minted}
	\caption{Pseudocode of the \Verb|toPolarsDataTypeFunction|}
	\label{lst:toPolarsDataTypeWithLogs}
\end{listing}

% \begin{figure}
% 	\begin{plantuml}
% 		@startuml
% 		interface ColumnDefinitionEntry {
% 				sheetColumnIndex: number
% 				columnName: string
% 				valueType: ValueType
% 				astNote: ValuetypeAssignment
% 			}
% 		@enduml
% 	\end{plantuml}
% 	\caption{\Verb|ColumnDefinitionEntry|}\label{fig:uml:ColumnDefinitionEntry}
% \end{figure}

\Verb|deriveColumnDefinitionEntriesWithoutHeader|, \Verb|deriveColumnDefinitionEntriesFromHeader| and \Verb|parseAndValidateValue| remain unchanged because they do not rely on the table implementation.

% - deriveColumnDefinitionEntriesWithoutHeader
% - uses the definitions from the schema property of the TableInterpreter block
% - return ColumnDefinitionEntry containing columnname and valueType
%
% - deriveColumnDefinitionEntriesFromHeader
% - use names in the header row to find column index of the header
% - if name isn't in the header row the column is ignored
% - otherwise same behavior as deriveColumnDefinitionEntriesWithoutHeader

\Verb|doExecute| only preprocesses the block's properties and leaves the implementation specifics to \Verb|TableInterpreterExecutor|'s subclasses (\ref{lst:tableInterpreter:doExecute}).
\begin{listing}
	\begin{minted}{python}
def doExecute(inputSheet, context):
	header = context.get("header")
	columnDefinitions = context.get("columns")
	if header:
		columnEntries = deriveColumnDefinitionEntriesFromHeader(columnDefinitions, inputSheet.header_row)
	else:
		columnEntries = deriveColumnDefinitionEntriesWithoutHeader(columnDefinitions)

	return constructAndValidateTable(
		inputSheet,
		header,
		columnEntries,
		context
	)
	\end{minted}
	\caption{Pseudocode for \Verb|TableInterpreter.doExecute|}
	\label{lst:tableInterpreter:doExecute}
\end{listing}

\subsubsection{PolarsTableInterpreterExecutor}
\label{subsubsection:polarstableinterpreterexecutor}
The static attribute \Verb|type| set to \Verb|'PolarsTableInterpreter'|.
This the value the interpreter checks when looking for block executors.
Because we manipulated the selection process in \ref{subsection:createBlockExecutor}, the interpreter is looking for \Verb|PolarsTableInterpreter| instead of \Verb|TableInterpreter|.

We outline the behavior of \Verb|constructAndValidateTable| (\ref{lst:polarsTableInterpreter:constructTable}) and \Verb|constructSeries| (\ref{lst:polarsTableInterpreter:constructSeries}) pseudocode examples in the respective listings.
\begin{listing}
	\begin{minted}{python}
def constructAndValidateTable(sheet, header, columnEntries, context):
	rows = sheet.getData()
	# rows is a row-oriented, two dimensional array of strings
	if header:
		... # Skip first row
	series = []  # Empty list
	for entry in columnEntries:
		s = this.constructSeries(rows, entry, context)
		series.add(s)
	dataFrame = polars.DataFrame(series)
	return new PolarsTable(dataFrame, context.valueTypeProvider)
	\end{minted}
	\caption{}
	\label{lst:polarsTableInterpreter:constructTable}
\end{listing}
\begin{listing}
	\begin{minted}{python}
def constructSeries(rows, columnEntry, context):
	valueType = columnEntry.valueType
	dataType = toPolarsTypeWithLogs(valueType, context.logger)
	values = []  # Empty list
	for row in rows:
		cell = row[columnEntry.sheetColumnIndex]
		value = this.parseAndValidateValue(cell, valueType, context)
		columnData.add(value)

	return polars.Series(columnEntry.ColumnName, values, dataType)
	\end{minted}
	\caption{}
	\label{lst:polarsTableInterpreter:constructSeries}
\end{listing}

% \subsubsection{TsTableInterpreter}
% - many of the TableInterpreter functions used to be here
% - no functionality added or changed

\section{FileToTableIntepreter}
The blocktype definition located at \Verb|libs/language-server/src/stdlib/builtin-block-types/FileToTableInterpreter.jv| defines the following properties:
FIXME: HERE


- didn't exist prior
- only has a polars implementation

\subsection{FileToTableIntepreterExecutor}
\Verb|colsAndSchema| returns a list of column names and a map from those column names to a Polars DataType.
The method reuses as much code from
- reuse code from tableInterpreter
\begin{listing}
	\begin{minted}{python}
def colsAndSchema(context):
	valueTypeAssignments = context.get('columns')
	columnDefinitionEntries = TableInterpreter.deriveColumnDefnitionEntriesWithoutHeader(valueTypeAssignments, context)
	schema = {}  # Empty map
	columnNames = []  # Empty list
	for columnDefinition in columnDefinitionEntries:
		# imported from table-interpreter-executor.ts
		schema[columnDefinition.columnName] = toPolarsDataTypeWithLogs(columnDefinition.valueType, context.logger)
		columnNames.add(columnDefinition.columnName)

	return {
		columnNames: columnNames,
		schema: schema,
	}
end
	\end{minted}
\end{listing}

- csvOptions
- returns a ReadCSVOptions object, that can be passed to polars' readCSV function.
- uses colsAndSchema
- other properties are just taken from context: header, delimiter, enclosing
- encoding is always utf8, throws error if not

- doExecute
- create ReadCSVOptions
- call constructAndValidateTable (echo structure of tableInterpreter)

- constructAndValidateTable
- get dataFrame from calling \Verb|polars.readCSV(content, options)|
- call PolarsTable constructor with dataframe and valuetypeprovider

\section{LocalFileToTableIntepreter}
- "no circular dependencies" rule prevents code sharing with LocalFileExtractor -> copy logic.
- by which I mean 1. get 'filePath' 2. check filePath does not include '..' (path traversal is restricted)
- use the static method csvOptions of PolarsTableInterpreterExecutor to reuse the code there and get csv parser options
- leave all of the actual filereading to polars.readCSV


\section{TableTransformer}
- again strategy.
- TsTableTransformer contains old behavoir
- move some helper methods logColumnOverwreiteStatus, checkInputColumnsExists to abstract class.

\subsection{PolarsTableTransformer}
- doExecute
- check input columns exist
- create transform executor (see relevant subsection)
- use checkInputColumnsMatchTransformInputTypes to get a map from input name to column expression (see section polars expression?)
- call executeTransform with this map -> returns pl.Expr
- extend expr to rename the resulting column to the specified name
- use Expr to add a column to the input table, or overwrite an existing one.


- checkInputColumnsMatchTransformInputTypes
\begin{listing}
	\begin{minted}{python}
function checkInputColumnsMatchTransformTypes(inputColumnNames, inputTable, transformInputDetailsList)
	is = range from 0 to inputColumnNames.length
	variableToColumnMap = {}
	for i in is do
		inputColumName = inputColumnNames[i]
		inputColumn = inputTable.getColumn(inputColumnName)
		inputDetails = transformInputDetailsList[i]

		if (
			!inputColumn.valueType.isConvertibleTo(inputDetails.valueType)
		) {
			return Error
		}
		variableName = inputDetails.name
		variableToColumnMap.set(variableName, pl.col(inputColumn.name))
	done
	return variableToColumnMap
	\end{minted}
\end{listing}
- pl.col selects the column with the name from a dataframe.


\section{Transforms}
\label{section:impl:transforms}
- previos tranform execution
1. pretend new table with input columns
2. execute expression for each row of that (pretend) table
3. add resulting column (transforms only allow for one result): if outputname is the same as another column that one gets replaced
- polars expressions are the new way

- addvariables to context
- context contains a map of references and their values
- add the variableToColumn map is added to context map. operator evaluator will be able to access these mappings

- doExecuteTransform
\begin{listing}
	\begin{minted}{lua}
function doExecuteTransform(variableToColumnName, context)
	inputDetails = this.getInputDetails()
	outputDetails = this.getOutputDetails()
	addInputColumnsToContext(inputDetails, variableToColumnName, context.evaluationContext)

	try {
		expr = polarsEvaluateExpression(this.getOutputAssignment().expression,
		context.evaluationContext)
	} catch (e) {
		log error
		return
	}
	return expr.cast(outputDetails.valueType.toPolarsDataType())
end
	\end{minted}
\end{listing}
- the .cast call casts all values in the resulting column to the desired type (if possible)

\subsection{Expressions}
- polarsEvaluateExpression:
\begin{figure}
	\begin{plantuml}
		@startuml
		start
		if (""expr"" is a free variable) then (yes)
		:retrieve the ""value"" of ""expr"" from context;
		if (""value"" is ""InternalValueRepresentation"") then (yes)
		:return ""pl.lit(value)"";
		stop
		else (no)
		:return ""value"";
		stop
		endif
		elseif (""expr"" is a valueLiteral) then (yes)
		:evaluate the ""value"" of ""expr"";
		if (""value === undefined"") then (true)
		:return ""undefined"";
		stop
		else (false)
		:return ""pl.lit(value)"";
		stop
		endif
		else
		:use the expressions n-ness (unary, binary, tertiary) and its
		operator to get the fitting evaluator from ""evaluationContext"";
		:call the ""eval.polarsEvaluate"" method;
		:return the result of that;
		stop
		endif
		@enduml
	\end{plantuml}
	\caption{
		Activity diagram of the \Verb|polarsEvaluateExpression| function.
		\Verb|expr| is the expression that should be evaluated. %FIXME: caption
	}
	\label{fig:uml:polars_evaluate_expression}
\end{figure}


\subsubsection{Operator evaluators}
\label{subsubsection:impl:operator_evaluator}
\begin{listing}
	\begin{minted}{typescript}
class MultiplicationOperatorEvaluator extends DefaultBinaryOperatorEvaluator<
  number,
  number,
  number
> {
  constructor() {
    super('*', NUMBER_TYPEGUARD, NUMBER_TYPEGUARD);
  }
  override doEvaluate(leftValue: number, rightValue: number): number {
    return leftValue * rightValue;
  }
  override polarsDoEvaluate(
    left: PolarsInternal,
    right: PolarsInternal,
  ): PolarsInternal {
    return left.mul(right);
  }
}
	\end{minted}

\end{listing}





- Operator type calculators were not changed
- they dont reflect the actual types of the data during runtime anymore.
- they still prevent the user from doing illegal stuff.


\subsubsection{example}
- imagune a jayvee pipeline containing this tabletransformer \
\begin{minted}{typescript}
	transform tr {
		from x oftype integer;
		from y oftype integer;
		to z oftype decimal;
		z: x * 2 + y;
	}
	block Bl oftype TableTransformer {
		inputColumns: ['a', 'b'];
		outputColumn: 'c';
		uses: tr;
	}
\end{minted}
- how he inputTable to Bl is transformed is depicted in this diagram
\begin{figure}
	\begin{plantuml}
		@startuml
		autoactivate on
		->":PolarsTableTransformerExecutor": execute(inputTable, transform)
		":PolarsTableTransformerExecutor" -> ":PolarsTransformExecutor": new PolarsTransformExecutor(transform)
		return PolarsTransformExecutor
		":PolarsTableTransformerExecutor" -> ":PolarsTransformExecutor": executeTransform({x: pl.col('a'), y:pl.col('b')})
		":PolarsTransformExecutor" -> ":EvaluationContext": setValueForReference('x', pl.col('a'))
		return
		":PolarsTransformExecutor" -> ":EvaluationContext": setValueForReference('y', pl.col('b'))
		return
		":PolarsTransformExecutor" -> ":AdditionOperatorEvaluator": x * 2 + y
		":AdditionOperatorEvaluator" -> ":MultiplicationOperatorEvaluator": x * 2
		":MultiplicationOperatorEvaluator" -> ":EvaluationContext": x
		return pl.col('a')
		":MultiplicationOperatorEvaluator" ->]: evaluateValueLiteral(2)
		return pl.lit(2)
		return pl.col('a').mul(pl.lit(2))
		":AdditionOperatorEvaluator" -> ":EvaluationContext": y
		return pl.col('b')
		return pl.col('a').mul(pl.lit(2)).add(pl.col('b'))
		return pl.col('a').mul(pl.lit(2)).add(pl.col('b')).cast(pl.f64)
		return \n pl.col('a').mul(pl.lit(2)).add(pl.col('b')).cast(pl.f64).alias('c')
		@enduml
	\end{plantuml}
	\caption{}
	\label{fig:uml:epr:example}
\end{figure}




\section{SQLiteLoader}
- doExecute
- retrieve file, table and dropTable properties from context
- call executeLoad

- executeLoad
- general implementation for both Ts and Polars.
- uses generateInsertTableStatement and generateCreateTableStatement from Table class, which have different implementations for polars and ts (see those sections)

- \Verb|PolarsSQLiteLoaderExecutor| and \Verb|TsSQLLoader| don't override the default implementation

\subsection{sqlite-loader-rust}
\label{subsection:sqlite-loader-rust}

\subsubsection{Napi-rs}
- napi-rs allows us to compile a rust library to a .node file
- nodejs can load and use this .node file
- the types that can be passed to this function is restricted by napi.rs capabilites

- we want to evaluate the extensibility of the polars backend
- create a extenal rust library with typescript interface via napi-package-template: \Verb|sqlite-loader-rust|
- napi function cannot have dataframe as a parameter: type is too complex for napi
- Solution: export dataframe into an arrow ipc file on disk. then
- call \Verb|loadSqlite|
- negative: context (including the logger, is lost)

\subsubsection{Ecosystem overview}
- rusqlite: no arrow support, but works well, can write
- arrow\_adbc: rust implementation is not there yet, it only has a dummy driver (not sqlite or postgres), requires dynamic linking of c libraries,
- connector\_arrow: built in rust, uses the arrow crate under the hood, supports many backends (postgres, duckdb, etc.), only sqlite implemented because prototype. WE USE THIS

\subsubsection{impl}
- in case of unrecoverable error, use napi's errors to throw an error in the typescript code. include messages by library errors.
- biggest reason for linecount and complexity is error handling

- ipcReader
- opens a file and wrape it in an arrow file reader
- use arrow::FileReader to read the ipc file into a BatchRecord iterator, because tables can be comprised of multiple batchrecords

- pop_first_batch: remove the first batch read from the iterator

- db_connection
- create a connection to a sqlite database using rusqlite
- wrap that connection with connector\_arrow's SQLiteConnection type
- can now write arrow data to this connection

- append: see listing
\begin{listing}
	\begin{minted}{lua}
function append(appender, first_batch, remainder)
	inserted_rows = 0
	remainder.prepend(first_batch)
	for batch in remainder do
		appender.append(batch)
		inserted_rows += batch.number_of_rows
	end
	return inserted_rows
end
	\end{minted}
\end{listing}

- load\_sqlite
\begin{listing}
	\begin{minted}{lua}
function load_sqlite(ipc_path, table_name, sqlite_path, drop_table)
	reader = ipc_reader()
	conn = db_connection(sqlite_path)

	if drop_table then
		conn.table_drop(table_name)
	endif

	schema = reader.schema()
	appender = conn.append(table_name)
	inserted_rows = append(appender, reader)
end
	\end{minted}
\end{listing}

