\chapter{Implementation}
\label{chapter:Implementation}

\section{Type Conversion}
- dtype -> vtype: add \Verb|fromPolarsDType| method to \Verb|ValueTypeProvider|. unsupported plds throw an error.
- unsupported: pl.Categorical, pl.Date, pl.DateTime, pl.List, pl.Null, pl.Struct
- vtype -> dtype: add \Verb|toPolarsDataType| method to interface \Verb|ValueType|. unsupported vts return undefined.
- unsupported: EmptyCollection, collection(if inner is unsupported), valuetype-assignment, transform, regex, constraint, cell-range

- InternalValueRepresentation
- gets new typeguards
% TODO: better explanation
- string, number, boolean: typeof
- RegExp: instanceof
- CellRangeLiteral, ConstraintDefintion, Valuetypeassignment, blocktype property, transform definition: langiun generated function
- atomicvaluerepresentation: any of the above
- internalarraytypeguard: node:isArray -> then every value fulfills internalvaluerepresentationtypeguard
- internalvaluerepresentationtypeguard: atomic or array
- prev: could only be used to narrow down internalvaluerepresentation
- now: unknwon types can be tested
- pl.Expr should be an atomicinternalvaluerepresentation but is kept seperate for allow the ts backend to coexist


\section{Table}
\subsection{abstract Table}
Jayvee implements \Verb{IOTypeImplementation<IOType.TABLE>} in \Verb{libs/execution/src/lib/types/io-types/table.ts}.
To increase clarity, the abstract \Verb|TableColumn| class and its subclasses \Verb|PolarsTableColumn| and \Verb|TsTableColumn| are moved into their own file \Verb|table-column.ts|.

- \Verb|Table| defines defines abstract methods for its subclasses to implement and directly implements general behaviour in static methods. %TODO?: code snippet here?
- generateDropTableStatement is a string with the tablename.




\subsection{PolarsTable}
It wraps the \Verb|polars.DataFrame| class and uses its methods to implement the abstract methods from \Verb|Table|, except with the general abstract types like \Verb|TableColumn| or \Verb|Table| replaced with types interacting with polars like \Verb|PolarsTable| or \Verb|PolarsTableColumn|.
- adapter pattern

The Valuetype will have to be reinferred anytime a column is requested. %FIXME: getColumn() is called.
\begin{listing}
	\begin{minted}{typescript}
override withColumn(column: PolarsTableColumn): PolarsTable {
  const ndf = this.df.withColumn(column.series);
  return new PolarsTable(ndf, this.valueTypeProvider);
}
	\end{minted}
	\caption{An exemplary implementation in PolarsTable}
	\label{lst:polarstable_impl_example}
\end{listing}

\section{TableColumn}

- The constructor accepts both valuetype and valuetypeProvider
- if it's a valuetypeprovider the valuetype is inferred from the series dtype with the \Verb{fromPolarsDType} method.
- \Verb|nth| has to cast \Verb|any| to \Verb|InternalValueRepresentation| using \Verb|INTERNAL_VALUE_REPRESENTATION_TYPEGUARD| (new).
- it return \Verb|null| in case of the value being \Verb|null|
- most methods map more ot less directly onto series methods.

- the \Verb|TsTableColumn| implementation is also new
- now it has a \Verb|name| attribute.
% maybe TODO?: plantuml diag.
- compared to polarstablecolumn:
- clone is hacky
- everything gets serialized to json -> deserialize into object -> reassert type information
- this is because normal nodejs deep clone looses type information
- push and drop are supported, because they are required by the typescript blocks


\section{new executors}
- we insert if statements into getExecutorForBlockype to change what BlockExecutorClass type is being searched for.




\subsection{TableInterpreter} %TODO
static methods


\section{FileToTableIntepreter}

\section{LocalFileToTableIntepreter}
- no circular dependencies rule prevents code sharing with LocalFileExtractor -> copy logic.
- use the static method csvOptions of PolarsTableInterpreterExecutor to reuse the code there and get csv parser options
- leave all of the actual filereading to polars


\section{TableTransformer}
- again strategy.
- steps:
1. check input coluns exist
2. create transform executor
3. check input columns match the transform input types.
3.1 compare the nth input column type to the nth transform input type and check with \Verb|isConvertibleTo|
3.1 return variableToColumnMap (varname -> columnexpr)
%maybee TODO: listing
4. execute the transform with the variabletocolumnmap -> returns polars expr that will perform the transform if applied to a table.
- wierd naming is to remain similar to the typescript implementation
5. extend the expression to include a renaming of the new column to \Verb|outputColumn|
6. apply the expression to the input table and return the resulting table

\subsection{Transforms}
- again strategy.
- previos tranform execution
1. pretend new table with input columns
2. execute expression for each row oth that (pretend) table
3. add resulting column (transforms only allow for one result): if outputname is the same as another column that one gets replaced

\subsubsection{Expressions}
- polarsEvaluateExpression:
- 1. free variable -> retrieve from context, if InternalValueRepresentation wrap in pl.lit and return.
- 2. literal -> wrap in pl.lit and return
- 3. other -> get evaluator from context / DefaultOperatorRegistry, evaluateExpression, return resulting pl.Expr or undefined
\begin{figure}
	\begin{plantuml}
		@startuml
		start
		if (""expr"" is a free variable) then (yes)
		:retrieve the ""value"" of ""expr"" from context;
		if (""value"" is ""InternalValueRepresentation"") then (yes)
		:return ""pl.lit(value)"";
		stop
		else (no)
		:return ""value"";
		stop
		endif
		elseif (""expr"" is a valueLiteral) then (yes)
		:evaluate the ""value"" of ""expr"";
		if (""value === undefined"") then (true)
		:return ""undefined"";
		stop
		else (false)
		:return ""pl.lit(value)"";
		stop
		endif
		else
		:use the expressions n-ness (unary, binary, tertiary) and its
		operator to get the fitting evaluator from ""evaluationContext"";
		:call the ""eval.polarsEvaluate"" method;
		:return the result of that;
		stop
		endif
		@enduml
	\end{plantuml}
	\caption{
		Activity diagram of the \Verb|polarsEvaluateExpression| function.
		\Verb|expr| is the expression that should be evaluated. %FIXME: caption
	}
	\label{fig:uml:polars_evaluate_expression}
\end{figure}




% TODO:
% @startuml
% ->BlockExecutor: execute()
% BlockExecutor -> AbstractBlockExecutor: execute()
% AbstractBlockExecutor -> PolarsTableTransformer: doExecute()
% PolarsTableTransformer -> PolarsTransformExecutor: new PolarsTransformExecutor()
% PolarsTransformExecutor -> PolarsTableTransformer: PolarsTableTransformer
% PolarsTableTransformer -> TransformExecutor: executeTransform()
% TransformExecutor -> PolarsTransformExecutor: doExecuteTransform()
% PolarsTransformExecutor -> : polarsEvaluateExpression()
% -> PolarsTransformExecutor: polars.polarsExpr
% PolarsTransformExecutor -> PolarsTableTr: idk
% @enduml

- Operator type calculators were not changed
- they dont reflect the actual types of the data during runtime anymore.
- they still prevent the user from doing illegal stuff.




\section{SQLiteLoader}
- generateDropTableStatement
- generateCreateTableStatement
- use sqlvisitor to get column definitions
- generateInsertTableStatement
- use `df.rows`
- map over rows
- zip values in row with their valuetype obtained -> use sqlvisitor to get sql formatted representation of the value
- join all values into a string for each row
- join all rows into a valid sql statement
- return
- add polars functionality with \Verb|PolarsSQLiteLoaderExecutor|
- \Verb|PolarsSQLiteLoaderExecutor| and \Verb|TsSQLLoader| don't override the default implementation

\subsection{Rust stuff}
- we want to evaluate the extensibility of the polars backend
- create a extenal rust library with typescript interface via napi-package-template: \Verb|sqlite-loader-rust|
- use the rust version of polars inside the rust library
- napi functions cannot have dataframe as a parameter %TODO: why
- export dataframe into an arrow ipc file on disk.
- call \Verb|loadSqlite|
- context (including the logger, is lost)

\subsubsection{Ecosystem overview}
- rusqlite: no arrow support, but works well, can write
- arrow\_adbc: rust implementation is not there yet, it only has a dummy driver (not sqlite or postgres), requires dynamic linking of c libraries,
- connector\_arrow: built in rust, uses the arrow crate under the hood, supports many backends (postgres, duckdb, etc.), only sqlite implemented because prototype.

- rust:
- in case of unrecoverable error, use napi's errors to throw an error in the typescript code. include messages by library errors.
- use arrow::FileReader to read the ipc file into a BatchRecord iterator, because tables can be comprised of multiple batchrecords
- use rusqlite library to create a sqlite connection
- pass connection to connector\_arrow,
- use \Verb|SQLiteConnection|'s methods \Verb|table_drop| and \Verb|table_create| (get the schema from the first batch)
- from connection, use \Verb|append| to prepare an appender for the table
- append all batches
- finish appender









