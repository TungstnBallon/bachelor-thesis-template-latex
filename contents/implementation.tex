\chapter{Implementation}
\label{chapter:Implementation}

\section{Type Conversion}
- dtype -> vtype: add \Verb|fromPolarsDType| method to \Verb|ValueTypeProvider|. unsupported plds throw an error.
- unsupported: pl.Categorical, pl.Date, pl.DateTime, pl.List, pl.Null, pl.Struct
- vtype -> dtype: add \Verb|toPolarsDataType| method to interface \Verb|ValueType|. unsupported vts return undefined.
- unsupported: EmptyCollection, collection(if inner is unsupported), valuetype-assignment, transform, regex, constraint, cell-range

- InternalValueRepresentation
- gets new typeguards
% TODO: better explanation
- string, number, boolean: typeof
- RegExp: instanceof
- CellRangeLiteral, ConstraintDefintion, Valuetypeassignment, blocktype property, transform definition: langiun generated function
- atomicvaluerepresentation: any of the above
- internalarraytypeguard: node:isArray -> then every value fulfills internalvaluerepresentationtypeguard
- internalvaluerepresentationtypeguard: atomic or array
- prev: could only be used to narrow down internalvaluerepresentation
- now: unknwon types can be tested
- pl.Expr should be an atomicinternalvaluerepresentation but is kept seperate for allow the ts backend to coexist


\section{Table}
\subsection{abstract Table}
Jayvee implements \Verb{IOTypeImplementation<IOType.TABLE>} in \Verb{libs/execution/src/lib/types/io-types/table.ts}.
To increase clarity, the abstract \Verb|TableColumn| class and its subclasses \Verb|PolarsTableColumn| and \Verb|TsTableColumn| are moved into their own file \Verb|table-column.ts|.

- \Verb|Table| defines defines abstract methods for its subclasses to implement and directly implements general behaviour in static methods. %TODO?: code snippet here?
- generateDropTableStatement is a string with the tablename.




\subsection{PolarsTable}
It wraps the \Verb|polars.DataFrame| class and uses its methods to implement the abstract methods from \Verb|Table|, except with the general abstract types like \Verb|TableColumn| or \Verb|Table| replaced with types interacting with polars like \Verb|PolarsTable| or \Verb|PolarsTableColumn|.
- adapter pattern

- "wrapper" functions:
- writeIpc, writeIpc
- withColumn, withColumnFromInternal
- nRows, nColumns
- clone
- tostring

- getTypes uses this.valueTypeProvider.formPolarsDType

- generateInsertValuesStatement
- use \Verb|SQLValueRepresentationVisitor| and valuetype to get the cell representation
- if cell is not internalvaluerepresentation (can be any in polars) use \mintedinline{typescript}|'NULL'|
- join cells using template literal / template string and \mintedinline{typescript}|Array<string>.join|
- join rows using and \mintedinline{typescript}|Array<string>.join|
- use template literal for final sql statement

- generateCreateTableStatement
- uses SQLColumnTypeVisitor to create convert ValueTypes to SQL Types e.g. 'decimal' becomes 'real'.

- hasColumn tries to get the column: no exception -> true, catch -> false

- columns: use this.valueTypProvider to recreate the valuetype

- getColumn:
- try/catch
- this.valueTypeProvider

- getRow:
- polars.DataFrame.row(id) returns any[] -> have to typeguard row, otherwise throw error

- acceptVisitor calls visitor.visitPolarsTable(this)

- isPolars returns True isTypescript returns False

\subsubsection{TsTable}
- attributes:
- number of rows: ensure rectangular shape
- _columns: map<String, TsTableColumn>: underscore to not intrefere with columns getter

- withColumn
- replaces addColumn: copy instead of mut
- clones prevoius table and sets the "name" entry to ncol (pseoudocode maybe?)

- columns returns the new TsTableColumn

- clone:
- create new map
- clone individual columns, wrap map in new table


\section{TableColumn}
\subsection{polarsTableColumn}
- wraps polars.Series
- constructor
- if given a valuetype its just set, if given a valuetypeprovider that ones fromPolarsDType function is used to do this

- wrappers:
- valueType, name, length, clone, series

- nth has to guard the type because polars.series.getIndex() returns any
- null if field is 'NULL'
- throw error on unexpected type


\subsubsection{TsTableColumn}
- properties are prefixed with underscored to allow for the getters to have the appropriate names.
- name is now a property (wan't in the old TableColumn interface)
- has a generic type T compared to PolarsTableColumn:
- pl.Series doesn't offer a generic type parameter, so PolarsTableColumn doesn't offer one either.
- T is the valuetype contained in the column
- wraps an array

- wrapper functions:
- valueType, name, length, nth (wraps at),
- \Verb|structuredClone| function loses type information (citation: programm crashes):
- hacky solution:
- stringify everything to json
- reparse it
- reassert types using typeguards
- don't do this to this.valueType. it must remain the same object to successfully check for equality

- drop uses splice
- push checks \_valueType.isInternalValueRepresentation before pushing

- The constructor accepts both valuetype and valuetypeProvider
- if it's a valuetypeprovider the valuetype is inferred from the series dtype with the \Verb{fromPolarsDType} method.
- \Verb|nth| has to cast \Verb|any| to \Verb|InternalValueRepresentation| using \Verb|INTERNAL_VALUE_REPRESENTATION_TYPEGUARD| (new).
- it return \Verb|null| in case of the value being \Verb|null|
- most methods map more ot less directly onto series methods.

- the \Verb|TsTableColumn| implementation is also new
- now it has a \Verb|name| attribute.
% maybe TODO?: plantuml diag.
- compared to polarstablecolumn:
- clone is hacky
- everything gets serialized to json -> deserialize into object -> reassert type information
- this is because normal nodejs deep clone looses type information
- push and drop are supported, because they are required by the typescript blocks


\section{new executors}
- we insert if statements into getExecutorForBlockype to change what BlockExecutorClass type is being searched for.
\begin{figure}
	\begin{plantuml}
		@startuml
		!pragma useVerticalIf on
		start
		if (blockTypeName === 'TableInterpreter') then (true)
		if (usePolars) then (true)
		:blockTypeName = 'PolarsTableInterpreter';
		else (false)
		:blockTypeName = 'TsTableInterpreter';
		endif
		elseif (blockTypeName === 'TableTransformer') then (true)
		if (usePolars) then (true)
		:blockTypeName = 'PolarsTableTransformer';
		else (false)
		:blockTypeName = 'TsTableTransformer';
		endif
		elseif (blockTypeName === 'SQLiteLoader') then (true)
		if (useRusqlite) then (true)
		:blockTypeName = 'RustTableTransformer';
		elseif (usePolars) then (true)
		:blockTypeName = 'PolarsTableTransformer';
		else (false)
		:blockTypeName = 'TsTableTransformer';
		endif
		endif
		stop
		@enduml
	\end{plantuml}
	\caption{How we pick the right blocktype name}\label{fig:uml:getExecutorForBlockType}
\end{figure}


\subsection{TableInterpreter} %TODO
- much of the functionality is implemented in static methods in the abstract class,
- both subclasses can use this functionality
- other classes that can import TableInterpreter can use it too.

\begin{figure}
	\begin{plantuml}
		@startuml
		interface ColumnDefinitionEntry {
				sheetColumnIndex: number
				columnName: string
				valueType: ValueType
				astNote: ValuetypeAssignment
			}
		@enduml
	\end{plantuml}
	\caption{\Verb|ColumnDefinitionEntry|}\label{fig:uml:ColumnDefinitionEntry}
\end{figure}


- deriveColumnDefinitionEntriesWithoutHeader
- uses the definitions from the schema property of the TableInterpreter block
- return ColumnDefinitionEntry containing columnname and valueType

- deriveColumnDefinitionEntriesFromHeader
- use names in the header row to find column index of the header
- if name isn't in the header row the column is ignored
- otherwise same behavior as deriveColumnDefinitionEntriesWithoutHeader

- doExecute
- \begin{listing}
	\begin{minted}{python}
function doExecute(inputSheet, context):
	header = context.get("columns")
	columnDefinitions = context.get("columns")
	if header then
		columnEntries = deriveColumnDefinitionEntriesFromHeader(columnDefinitions, inputSheet.header_row)
	else
		columnEntries = deriveColumnDefinitionEntriesWithoutHeader(columnDefinitions)
	endif
		return constructAndValidateTable(
			inputSheet,
			header,
			columnEntries,
			context
		)
	\end{minted}
	\caption{}
	\label{lst:tableInterpreter:doExecute}
\end{listing}
- subclasses only override the abstract constructAndValidateTable.

- parseAndValidateValue
- parses with preexisting parseValueToInternalRepresenation
- validates with preexisting isValidValueRepresentation

- export function toPolarsDataTypeWithLogs
- reusable by other block executors
- wrapper around ValueType.toPolarsDataType
- adds logging using Logger class unavailable for toPolarsDataType
- default is pl.Utf8 (string)

\subsection{PolarsTableInterpreterExecutor}
- type is 'PolarsTableInterpreter'

- constructAndValidateTable
- if header skip first sheet row
- for each columndefinitionentry call constructseries with a d2 array of all cells (row oriented)
- have list of series
- create df
- create PolarsTable
- hand valueTypeProvider to PolarsTable (see above)

- constructSeries
- recieve 2d array of rows + columndefinitionentry
- get polars dtype from toPolarsDataTypeWithLogs
\begin{listing}
	\begin{minted}{python}
function constructSeries(rows, columnEntry, context):
	dtype = toPolarsTypeWithLogs(columnEntry.valueType, context.logger)
	columnData = empty list
	for row in rows do
		cell = row[columnEntry.sheetColumnIndex]
		valueType = columnEntry.valueType
		cell = parseAndValidateValue(cell, valueType, context)
		columnData.push(cell)
	done
	return polars.Series(columnEntry.ColumnName, columnData, dtype)
	\end{minted}
	\caption{}
	\label{lst:polarsTableInterpreter:constructSeries}
\end{listing}

\subsection{TsTableInterpreter}
- many of the TableInterpreter functions used to be here
- no functionality added or changed

\section{FileToTableIntepreter}
- didn't exist prior
- only has a polars implementation

- colsAndSchema
- satic so it can be reused
- returns a list of column names and a map from colame to polarstype

\section{LocalFileToTableIntepreter}
- no circular dependencies rule prevents code sharing with LocalFileExtractor -> copy logic.
- use the static method csvOptions of PolarsTableInterpreterExecutor to reuse the code there and get csv parser options
- leave all of the actual filereading to polars


\section{TableTransformer}
- again strategy.
- steps:
1. check input coluns exist
2. create transform executor
3. check input columns match the transform input types.
3.1 compare the nth input column type to the nth transform input type and check with \Verb|isConvertibleTo|
3.1 return variableToColumnMap (varname -> columnexpr)
%maybee TODO: listing
4. execute the transform with the variabletocolumnmap -> returns polars expr that will perform the transform if applied to a table.
- wierd naming is to remain similar to the typescript implementation
5. extend the expression to include a renaming of the new column to \Verb|outputColumn|
6. apply the expression to the input table and return the resulting table

\subsection{Transforms}
- again strategy.
- previos tranform execution
1. pretend new table with input columns
2. execute expression for each row oth that (pretend) table
3. add resulting column (transforms only allow for one result): if outputname is the same as another column that one gets replaced

\subsubsection{Expressions}
- polarsEvaluateExpression:
- 1. free variable -> retrieve from context, if InternalValueRepresentation wrap in pl.lit and return.
- 2. literal -> wrap in pl.lit and return
- 3. other -> get evaluator from context / DefaultOperatorRegistry, evaluateExpression, return resulting pl.Expr or undefined
\begin{figure}
	\begin{plantuml}
		@startuml
		start
		if (""expr"" is a free variable) then (yes)
		:retrieve the ""value"" of ""expr"" from context;
		if (""value"" is ""InternalValueRepresentation"") then (yes)
		:return ""pl.lit(value)"";
		stop
		else (no)
		:return ""value"";
		stop
		endif
		elseif (""expr"" is a valueLiteral) then (yes)
		:evaluate the ""value"" of ""expr"";
		if (""value === undefined"") then (true)
		:return ""undefined"";
		stop
		else (false)
		:return ""pl.lit(value)"";
		stop
		endif
		else
		:use the expressions n-ness (unary, binary, tertiary) and its
		operator to get the fitting evaluator from ""evaluationContext"";
		:call the ""eval.polarsEvaluate"" method;
		:return the result of that;
		stop
		endif
		@enduml
	\end{plantuml}
	\caption{
		Activity diagram of the \Verb|polarsEvaluateExpression| function.
		\Verb|expr| is the expression that should be evaluated. %FIXME: caption
	}
	\label{fig:uml:polars_evaluate_expression}
\end{figure}




% TODO:
% @startuml
% ->BlockExecutor: execute()
% BlockExecutor -> AbstractBlockExecutor: execute()
% AbstractBlockExecutor -> PolarsTableTransformer: doExecute()
% PolarsTableTransformer -> PolarsTransformExecutor: new PolarsTransformExecutor()
% PolarsTransformExecutor -> PolarsTableTransformer: PolarsTableTransformer
% PolarsTableTransformer -> TransformExecutor: executeTransform()
% TransformExecutor -> PolarsTransformExecutor: doExecuteTransform()
% PolarsTransformExecutor -> : polarsEvaluateExpression()
% -> PolarsTransformExecutor: polars.polarsExpr
% PolarsTransformExecutor -> PolarsTableTr: idk
% @enduml

- Operator type calculators were not changed
- they dont reflect the actual types of the data during runtime anymore.
- they still prevent the user from doing illegal stuff.




\section{SQLiteLoader}
- generateDropTableStatement
- generateCreateTableStatement
- use sqlvisitor to get column definitions
- generateInsertTableStatement
- use `df.rows`
- map over rows
- zip values in row with their valuetype obtained -> use sqlvisitor to get sql formatted representation of the value
- join all values into a string for each row
- join all rows into a valid sql statement
- return
- add polars functionality with \Verb|PolarsSQLiteLoaderExecutor|
- \Verb|PolarsSQLiteLoaderExecutor| and \Verb|TsSQLLoader| don't override the default implementation

\subsection{Rust stuff}
- we want to evaluate the extensibility of the polars backend
- create a extenal rust library with typescript interface via napi-package-template: \Verb|sqlite-loader-rust|
- use the rust version of polars inside the rust library
- napi functions cannot have dataframe as a parameter %TODO: why
- export dataframe into an arrow ipc file on disk.
- call \Verb|loadSqlite|
- context (including the logger, is lost)

\subsubsection{Ecosystem overview}
- rusqlite: no arrow support, but works well, can write
- arrow\_adbc: rust implementation is not there yet, it only has a dummy driver (not sqlite or postgres), requires dynamic linking of c libraries,
- connector\_arrow: built in rust, uses the arrow crate under the hood, supports many backends (postgres, duckdb, etc.), only sqlite implemented because prototype.

- rust:
- in case of unrecoverable error, use napi's errors to throw an error in the typescript code. include messages by library errors.
- use arrow::FileReader to read the ipc file into a BatchRecord iterator, because tables can be comprised of multiple batchrecords
- use rusqlite library to create a sqlite connection
- pass connection to connector\_arrow,
- use \Verb|SQLiteConnection|'s methods \Verb|table_drop| and \Verb|table_create| (get the schema from the first batch)
- from connection, use \Verb|append| to prepare an appender for the table
- append all batches
- finish appender









