\chapter{Implementation}
\label{chapter:Implementation}

\section{Type Conversion}
\label{subsection:TypeConversion}
- dtype -> vtype: add \Verb|fromPolarsDType| method to \Verb|ValueTypeProvider|. unsupported plds throw an error.
- unsupported: pl.Categorical, pl.Date, pl.DateTime, pl.List, pl.Null, pl.Struct
- vtype -> dtype: add \Verb|toPolarsDataType| method to interface \Verb|ValueType|. unsupported vts return undefined.
- unsupported: EmptyCollection, collection(if inner is unsupported), valuetype-assignment, transform, regex, constraint, cell-range

- InternalValueRepresentation
- gets new typeguards
% TODO: better explanation
- string, number, boolean: typeof
- RegExp: instanceof
- CellRangeLiteral, ConstraintDefintion, Valuetypeassignment, blocktype property, transform definition: langiun generated function
- atomicvaluerepresentation: any of the above
- internalarraytypeguard: node:isArray -> then every value fulfills internalvaluerepresentationtypeguard
- internalvaluerepresentationtypeguard: atomic or array
- prev: could only be used to narrow down internalvaluerepresentation
- now: unknwon types can be tested
- pl.Expr should be an atomicinternalvaluerepresentation but is kept seperate for allow the ts backend to coexist


\section{Table}
\subsection{abstract Table}
Jayvee implements \Verb{IOTypeImplementation<IOType.TABLE>} in \Verb{libs/execution/src/lib/types/io-types/table.ts}.
To increase clarity, the abstract \Verb|TableColumn| class and its subclasses \Verb|PolarsTableColumn| and \Verb|TsTableColumn| are moved into their own file \Verb|table-column.ts|.

- \Verb|Table| defines defines abstract methods for its subclasses to implement and directly implements general behaviour in static methods. %TODO?: code snippet here?
- generateDropTableStatement is a string with the tablename.

\subsubsection{changes compared to old table}
\label{subssubsection:impl:old_vs_new}
The \Verb|addColumn| method was replaced with \Verb|withColumn|.
- \Verb|TsTableInterpreter| still doesn't clone in \Verb|withColumn| to remain consistent.
- leads to less mutations, so it's better for parallel execution %TODO: citation




\subsection{PolarsTable}
It wraps the \Verb|polars.DataFrame| class and uses its methods to implement the abstract methods from \Verb|Table|, except with the general abstract types like \Verb|TableColumn| or \Verb|Table| replaced with types interacting with polars like \Verb|PolarsTable| or \Verb|PolarsTableColumn|.
- adapter pattern

- "wrapper" functions:
- writeIpc, writeIpc
- withColumn, withColumnFromInternal
- nRows, nColumns
- clone
- tostring

- getTypes uses this.valueTypeProvider.formPolarsDType

- generateInsertValuesStatement
- use \Verb|SQLValueRepresentationVisitor| and valuetype to get the cell representation
- if cell is not internalvaluerepresentation (can be any in polars) use \mintedinline{typescript}|'NULL'|
- join cells using template literal / template string and \mintedinline{typescript}|Array<string>.join|
- join rows using and \mintedinline{typescript}|Array<string>.join|
- use template literal for final sql statement

- generateCreateTableStatement
- uses SQLColumnTypeVisitor to create convert ValueTypes to SQL Types e.g. 'decimal' becomes 'real'.

- hasColumn tries to get the column: no exception -> true, catch -> false

- columns: use this.valueTypProvider to recreate the valuetype

- getColumn:
- try/catch
- this.valueTypeProvider

- getRow:
- polars.DataFrame.row(id) returns any[] -> have to typeguard row, otherwise throw error

- acceptVisitor calls visitor.visitPolarsTable(this)

- isPolars returns True isTypescript returns False

\subsubsection{TsTable}
- attributes:
- number of rows: ensure rectangular shape
- \_columns: map<String, TsTableColumn>: underscore to not intrefere with columns getter

- withColumn
- replaces addColumn: copy instead of mut
- clones prevoius table and sets the "name" entry to ncol (pseoudocode maybe?)

- columns returns the new TsTableColumn

- clone:
- create new map
- clone individual columns, wrap map in new table


\section{TableColumn}
\subsection{polarsTableColumn}
- wraps polars.Series
- constructor
- if given a valuetype its just set, if given a valuetypeprovider that ones fromPolarsDType function is used to do this

- wrappers:
- valueType, name, length, clone, series

- nth has to guard the type because polars.series.getIndex() returns any
- null if field is 'NULL'
- throw error on unexpected type


\subsubsection{TsTableColumn}
- properties are prefixed with underscored to allow for the getters to have the appropriate names.
- name is now a property (wan't in the old TableColumn interface)
- has a generic type T compared to PolarsTableColumn:
- pl.Series doesn't offer a generic type parameter, so PolarsTableColumn doesn't offer one either.
- T is the valuetype contained in the column
- wraps an array

- wrapper functions:
- valueType, name, length, nth (wraps at),
- \Verb|structuredClone| function loses type information (citation: programm crashes):
- hacky solution:
- stringify everything to json
- reparse it
- reassert types using typeguards
- don't do this to this.valueType. it must remain the same object to successfully check for equality

- drop uses splice
- push checks \_valueType.isInternalValueRepresentation before pushing

- The constructor accepts both valuetype and valuetypeProvider
- if it's a valuetypeprovider the valuetype is inferred from the series dtype with the \Verb{fromPolarsDType} method.
- \Verb|nth| has to cast \Verb|any| to \Verb|InternalValueRepresentation| using \Verb|INTERNAL_VALUE_REPRESENTATION_TYPEGUARD| (new).
- it return \Verb|null| in case of the value being \Verb|null|
- most methods map more ot less directly onto series methods.

- the \Verb|TsTableColumn| implementation is also new
- now it has a \Verb|name| attribute.
% maybe TODO?: plantuml diag.
- compared to polarstablecolumn:
- clone is hacky
- everything gets serialized to json -> deserialize into object -> reassert type information
- this is because normal nodejs deep clone looses type information
- push and drop are supported, because they are required by the typescript blocks


\section{new executors}

\subsection{createBlockExecutor}
\label{subsection:createBlockExecutor}
- we insert if statements into getExecutorForBlockype to change what BlockExecutorClass type is being searched for.
\begin{figure}
	\begin{plantuml}
		@startuml
		!pragma useVerticalIf on
		start
		if (blockTypeName === 'TableInterpreter') then (true)
		if (usePolars) then (true)
		:blockTypeName = 'PolarsTableInterpreter';
		else (false)
		:blockTypeName = 'TsTableInterpreter';
		endif
		elseif (blockTypeName === 'TableTransformer') then (true)
		if (usePolars) then (true)
		:blockTypeName = 'PolarsTableTransformer';
		else (false)
		:blockTypeName = 'TsTableTransformer';
		endif
		elseif (blockTypeName === 'SQLiteLoader') then (true)
		if (useRusqlite) then (true)
		:blockTypeName = 'RustTableTransformer';
		elseif (usePolars) then (true)
		:blockTypeName = 'PolarsTableTransformer';
		else (false)
		:blockTypeName = 'TsTableTransformer';
		endif
		endif
		stop
		@enduml
	\end{plantuml}
	\caption{How we pick the right blocktype name}\label{fig:uml:getExecutorForBlockType}
\end{figure}


\subsection{TableInterpreter} %TODO
\label{section:tableinterpreterexecutor}
- much of the functionality is implemented in static methods in the abstract class,
- both subclasses can use this functionality
- other classes that can import TableInterpreter can use it too.

\begin{figure}
	\begin{plantuml}
		@startuml
		interface ColumnDefinitionEntry {
				sheetColumnIndex: number
				columnName: string
				valueType: ValueType
				astNote: ValuetypeAssignment
			}
		@enduml
	\end{plantuml}
	\caption{\Verb|ColumnDefinitionEntry|}\label{fig:uml:ColumnDefinitionEntry}
\end{figure}


- deriveColumnDefinitionEntriesWithoutHeader
- uses the definitions from the schema property of the TableInterpreter block
- return ColumnDefinitionEntry containing columnname and valueType

- deriveColumnDefinitionEntriesFromHeader
- use names in the header row to find column index of the header
- if name isn't in the header row the column is ignored
- otherwise same behavior as deriveColumnDefinitionEntriesWithoutHeader

- doExecute
- retrieve and validate properties: header, columns
- parse columndefinitionentries, make useful
- leave table construction to \Verb|constructAndValidateTable|
- \begin{listing}
	\begin{minted}{python}
def doExecute(inputSheet, context):
	header = context.get("header")
	columnDefinitions = context.get("columns")
	if header:
		columnEntries = deriveColumnDefinitionEntriesFromHeader(columnDefinitions, inputSheet.header_row)
	else:
		columnEntries = deriveColumnDefinitionEntriesWithoutHeader(columnDefinitions)

	return constructAndValidateTable(
		inputSheet,
		header,
		columnEntries,
		context
	)
	\end{minted}
	\caption{}
	\label{lst:tableInterpreter:doExecute}
\end{listing}
- subclasses only override the abstract constructAndValidateTable.

- parseAndValidateValue
- parses with preexisting parseValueToInternalRepresenation
- validates with preexisting isValidValueRepresentation

- export function toPolarsDataTypeWithLogs
- reusable by other block executors
- wrapper around ValueType.toPolarsDataType
- adds logging using Logger class unavailable for toPolarsDataType
- default is pl.Utf8 (string)
- \Verb|toPolarsDataTypeWithLogs| is its own mehtod to enable code sharing with other table blocks.
- can't be in language-server because circular dependency

\subsubsection{PolarsTableInterpreterExecutor}
\label{subsubsection:polarstableinterpreterexecutor}
- type is 'PolarsTableInterpreter'

- constructAndValidateTable
- if header skip first sheet row
- for each columndefinitionentry call constructseries with a d2 array of all cells (row oriented)
- have list of series
- create df
- create PolarsTable
- hand valueTypeProvider to PolarsTable (see above)

- constructSeries
- recieve 2d array of rows + columndefinitionentry
- get polars dtype from toPolarsDataTypeWithLogs
\begin{listing}
	\begin{minted}{python}
function constructSeries(rows, columnEntry, context):
	dtype = toPolarsTypeWithLogs(columnEntry.valueType, context.logger)
	columnData = empty list
	for row in rows do
		cell = row[columnEntry.sheetColumnIndex]
		valueType = columnEntry.valueType
		cell = parseAndValidateValue(cell, valueType, context)
		columnData.push(cell)
	done
	return polars.Series(columnEntry.ColumnName, columnData, dtype)
	\end{minted}
	\caption{}
	\label{lst:polarsTableInterpreter:constructSeries}
\end{listing}

\subsubsection{TsTableInterpreter}
- many of the TableInterpreter functions used to be here
- no functionality added or changed

\section{FileToTableIntepreter}
- didn't exist prior
- only has a polars implementation

- colsAndSchema
- satic so it can be reused
- returns a list of column names and a map from colame to polarstype
- reuse code from tableInterpreter
\begin{listing}
	\begin{minted}{python}
function colsAndSchema(context)
	valueTypeAssignments = context.get('columns')
	columnDefinitionEntries = TableInterpreter.deriveColumnDefnitionEntriesWithoutHeader(valueTypeAssignments, context)
	schema = {}
	columnNames = []
	for columnDefinition in columnDefinitionEntries do
		// imported from table-interpreter-executor.ts
		schema[columnDefinition.columnName] = toPolarsDataTypeWithLogs(columnDefinition.valueType, context.logger)
		columnNames.push(columnDefinition.columnName)
	done

	return {
		columnNames: columnNames,
		schema: schema,
	}
end
	\end{minted}
\end{listing}

- csvOptions
- returns a ReadCSVOptions object, that can be passed to polars' readCSV function.
- uses colsAndSchema
- other properties are just taken from context: header, delimiter, enclosing
- encoding is always utf8, throws error if not

- doExecute
- create ReadCSVOptions
- call constructAndValidateTable (echo structure of tableInterpreter)

- constructAndValidateTable
- get dataFrame from calling \Verb|polars.readCSV(content, options)|
- call PolarsTable constructor with dataframe and valuetypeprovider

\section{LocalFileToTableIntepreter}
- "no circular dependencies" rule prevents code sharing with LocalFileExtractor -> copy logic.
- by which I mean 1. get 'filePath' 2. check filePath does not include '..' (path traversal is restricted)
- use the static method csvOptions of PolarsTableInterpreterExecutor to reuse the code there and get csv parser options
- leave all of the actual filereading to polars.readCSV


\section{TableTransformer}
- again strategy.
- TsTableTransformer contains old behavoir
- move some helper methods logColumnOverwreiteStatus, checkInputColumnsExists to abstract class.

\subsection{PolarsTableTransformer}
- doExecute
- check input columns exist
- create transform executor (see relevant subsection)
- use checkInputColumnsMatchTransformInputTypes to get a map from input name to column expression (see section polars expression?)
- call executeTransform with this map -> returns pl.Expr
- extend expr to rename the resulting column to the specified name
- use Expr to add a column to the input table, or overwrite an existing one.


- checkInputColumnsMatchTransformInputTypes
\begin{listing}
	\begin{minted}{python}
function checkInputColumnsMatchTransformTypes(inputColumnNames, inputTable, transformInputDetailsList)
	is = range from 0 to inputColumnNames.length
	variableToColumnMap = {}
	for i in is do
		inputColumName = inputColumnNames[i]
		inputColumn = inputTable.getColumn(inputColumnName)
		inputDetails = transformInputDetailsList[i]

		if (
			!inputColumn.valueType.isConvertibleTo(inputDetails.valueType)
		) {
			return Error
		}
		variableName = inputDetails.name
		variableToColumnMap.set(variableName, pl.col(inputColumn.name))
	done
	return variableToColumnMap
	\end{minted}
\end{listing}
- pl.col selects the column with the name from a dataframe.


\section{Transforms}
\label{section:impl:transforms}
- previos tranform execution
1. pretend new table with input columns
2. execute expression for each row of that (pretend) table
3. add resulting column (transforms only allow for one result): if outputname is the same as another column that one gets replaced
- polars expressions are the new way

- addvariables to context
- context contains a map of references and their values
- add the variableToColumn map is added to context map. operator evaluator will be able to access these mappings

- doExecuteTransform
\begin{listing}
	\begin{minted}{lua}
function doExecuteTransform(variableToColumnName, context)
	inputDetails = this.getInputDetails()
	outputDetails = this.getOutputDetails()
	addInputColumnsToContext(inputDetails, variableToColumnName, context.evaluationContext)

	try {
		expr = polarsEvaluateExpression(this.getOutputAssignment().expression,
		context.evaluationContext)
	} catch (e) {
		log error
		return
	}
	return expr.cast(outputDetails.valueType.toPolarsDataType())
end
	\end{minted}
\end{listing}
- the .cast call casts all values in the resulting column to the desired type (if possible)

\subsection{Expressions}
- polarsEvaluateExpression:
\begin{figure}
	\begin{plantuml}
		@startuml
		start
		if (""expr"" is a free variable) then (yes)
		:retrieve the ""value"" of ""expr"" from context;
		if (""value"" is ""InternalValueRepresentation"") then (yes)
		:return ""pl.lit(value)"";
		stop
		else (no)
		:return ""value"";
		stop
		endif
		elseif (""expr"" is a valueLiteral) then (yes)
		:evaluate the ""value"" of ""expr"";
		if (""value === undefined"") then (true)
		:return ""undefined"";
		stop
		else (false)
		:return ""pl.lit(value)"";
		stop
		endif
		else
		:use the expressions n-ness (unary, binary, tertiary) and its
		operator to get the fitting evaluator from ""evaluationContext"";
		:call the ""eval.polarsEvaluate"" method;
		:return the result of that;
		stop
		endif
		@enduml
	\end{plantuml}
	\caption{
		Activity diagram of the \Verb|polarsEvaluateExpression| function.
		\Verb|expr| is the expression that should be evaluated. %FIXME: caption
	}
	\label{fig:uml:polars_evaluate_expression}
\end{figure}


\subsubsection{Operator evaluators}
\label{subsubsection:impl:operator_evaluator}
\begin{listing}
	\begin{minted}{typescript}
class MultiplicationOperatorEvaluator extends DefaultBinaryOperatorEvaluator<
  number,
  number,
  number
> {
  constructor() {
    super('*', NUMBER_TYPEGUARD, NUMBER_TYPEGUARD);
  }
  override doEvaluate(leftValue: number, rightValue: number): number {
    return leftValue * rightValue;
  }
  override polarsDoEvaluate(
    left: PolarsInternal,
    right: PolarsInternal,
  ): PolarsInternal {
    return left.mul(right);
  }
}
	\end{minted}

\end{listing}





- Operator type calculators were not changed
- they dont reflect the actual types of the data during runtime anymore.
- they still prevent the user from doing illegal stuff.


\subsubsection{example}
- imagune a jayvee pipeline containing this tabletransformer \
\begin{minted}{typescript}
	transform tr {
		from x oftype integer;
		from y oftype integer;
		to z oftype decimal;
		z: x * 2 + y;
	}
	block Bl oftype TableTransformer {
		inputColumns: ['a', 'b'];
		outputColumn: 'c';
		uses: tr;
	}
\end{minted}
- how he inputTable to Bl is transformed is depicted in this diagram
\begin{figure}
	\begin{plantuml}
		@startuml
		autoactivate on
		->":PolarsTableTransformerExecutor": execute(inputTable, transform)
		":PolarsTableTransformerExecutor" -> ":PolarsTransformExecutor": new PolarsTransformExecutor(transform)
		return PolarsTransformExecutor
		":PolarsTableTransformerExecutor" -> ":PolarsTransformExecutor": executeTransform({x: pl.col('a'), y:pl.col('b')})
		":PolarsTransformExecutor" -> ":EvaluationContext": setValueForReference('x', pl.col('a'))
		return
		":PolarsTransformExecutor" -> ":EvaluationContext": setValueForReference('y', pl.col('b'))
		return
		":PolarsTransformExecutor" -> ":AdditionOperatorEvaluator": x * 2 + y
		":AdditionOperatorEvaluator" -> ":MultiplicationOperatorEvaluator": x * 2
		":MultiplicationOperatorEvaluator" -> ":EvaluationContext": x
		return pl.col('a')
		":MultiplicationOperatorEvaluator" ->]: evaluateValueLiteral(2)
		return pl.lit(2)
		return pl.col('a').mul(pl.lit(2))
		":AdditionOperatorEvaluator" -> ":EvaluationContext": y
		return pl.col('b')
		return pl.col('a').mul(pl.lit(2)).add(pl.col('b'))
		return pl.col('a').mul(pl.lit(2)).add(pl.col('b')).cast(pl.f64)
		return \n pl.col('a').mul(pl.lit(2)).add(pl.col('b')).cast(pl.f64).alias('c')
		@enduml
	\end{plantuml}
	\caption{}
	\label{fig:uml:epr:example}
\end{figure}




\section{SQLiteLoader}
- doExecute
- retrieve file, table and dropTable properties from context
- call executeLoad

- executeLoad
- general implementation for both Ts and Polars.
- uses generateInsertTableStatement and generateCreateTableStatement from Table class, which have different implementations for polars and ts (see those sections)

- \Verb|PolarsSQLiteLoaderExecutor| and \Verb|TsSQLLoader| don't override the default implementation

\subsection{sqlite-loader-rust}
\label{subsection:sqlite-loader-rust}

\subsubsection{Napi-rs}
- napi-rs allows us to compile a rust library to a .node file
- nodejs can load and use this .node file
- the types that can be passed to this function is restricted by napi.rs capabilites

- we want to evaluate the extensibility of the polars backend
- create a extenal rust library with typescript interface via napi-package-template: \Verb|sqlite-loader-rust|
- napi function cannot have dataframe as a parameter: type is too complex for napi
- Solution: export dataframe into an arrow ipc file on disk. then
- call \Verb|loadSqlite|
- negative: context (including the logger, is lost)

\subsubsection{Ecosystem overview}
- rusqlite: no arrow support, but works well, can write
- arrow\_adbc: rust implementation is not there yet, it only has a dummy driver (not sqlite or postgres), requires dynamic linking of c libraries,
- connector\_arrow: built in rust, uses the arrow crate under the hood, supports many backends (postgres, duckdb, etc.), only sqlite implemented because prototype. WE USE THIS

\subsubsection{impl}
- in case of unrecoverable error, use napi's errors to throw an error in the typescript code. include messages by library errors.
- biggest reason for linecount and complexity is error handling

- ipcReader
- opens a file and wrape it in an arrow file reader
- use arrow::FileReader to read the ipc file into a BatchRecord iterator, because tables can be comprised of multiple batchrecords

- pop_first_batch: remove the first batch read from the iterator

- db_connection
- create a connection to a sqlite database using rusqlite
- wrap that connection with connector\_arrow's SQLiteConnection type
- can now write arrow data to this connection

- append: see listing
\begin{listing}
	\begin{minted}{lua}
function append(appender, first_batch, remainder)
	inserted_rows = 0
	remainder.prepend(first_batch)
	for batch in remainder do
		appender.append(batch)
		inserted_rows += batch.number_of_rows
	end
	return inserted_rows
end
	\end{minted}
\end{listing}

- load\_sqlite
\begin{listing}
	\begin{minted}{lua}
function load_sqlite(ipc_path, table_name, sqlite_path, drop_table)
	reader = ipc_reader()
	conn = db_connection(sqlite_path)

	if drop_table then
		conn.table_drop(table_name)
	endif

	schema = reader.schema()
	appender = conn.append(table_name)
	inserted_rows = append(appender, reader)
end
	\end{minted}
\end{listing}

